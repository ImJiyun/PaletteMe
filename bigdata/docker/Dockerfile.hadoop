# Base image
FROM ubuntu:22.04

# Install dependencies (openssh-server + openssh-client 포함)
RUN apt-get update && \
    apt-get install -y wget curl openjdk-8-jdk python3 python3-pip \
      openssh-server openssh-client vim net-tools iputils-ping && \
    apt-get clean

# Java environment variables
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Hadoop installation
ENV HADOOP_VERSION=3.3.6
RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz && \
    tar -xvzf hadoop-${HADOOP_VERSION}.tar.gz && \
    mv hadoop-${HADOOP_VERSION} /usr/local/hadoop && \
    rm hadoop-${HADOOP_VERSION}.tar.gz

ENV HADOOP_HOME=/usr/local/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Spark installation
ENV SPARK_VERSION=3.4.1
ENV HADOOP_FOR_SPARK=3
RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_FOR_SPARK}.tgz && \
    tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_FOR_SPARK}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_FOR_SPARK} /usr/local/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_FOR_SPARK}.tgz

ENV SPARK_HOME=/usr/local/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# SSH setup: 생성한 키를 통해 비밀번호 없이 접속하도록 설정
RUN mkdir -p /root/.ssh && \
    ssh-keygen -t rsa -b 2048 -N "" -f /root/.ssh/id_rsa && \
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && \
    chmod 700 /root/.ssh && chmod 600 /root/.ssh/authorized_keys

# 기본적인 SSH 클라이언트 설정 (호스트 키 검증은 활성화)
RUN echo "Host *\n    StrictHostKeyChecking yes\n    UserKnownHostsFile ~/.ssh/known_hosts" > /root/.ssh/config

# SSH 데몬 포트 노출
EXPOSE 22

# 작업 디렉토리 설정
WORKDIR /root

# 설정 파일 및 엔트리포인트 스크립트 복사
COPY config/hadoop /config/hadoop
COPY config/spark /config/spark
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]