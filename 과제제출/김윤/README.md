# 1. Hadoop 버전

### Hadoop 버전

- 버전1 : HDFS + Map Reduce
- 버전2(2013년) : HDFS + Map Reduce + YARN
    - YARN 리소스 관리자에 위에 여러 분산 어플리케이션이 동작할 수 있는 환경이 마련됐다
    - 하둡이 대규모 분산 시스템을 구축하기 위한 플랫폼 역할 수행
    - hadoop ecosystem에 있는 프로그램들이 hadoop과는 독립적으로 개발되었기 대문에, 모든 프로그램이 hadoop에 의존하지는 않는다

### 버전별 차이점

- spark와 같은 여러 분산 어플리케이션을 YARN에 제출하여 동작 시킬 수 있다

<img src = "과제제출/김윤/asset/1.png" width="100%" height="100%">

### 버전별 아키텍쳐

> 버전1
> 

<img src = "과제제출/김윤/asset/2.png" width="100%" height="100%">


- 문제점
    - Namenode가 단일 장애지점(SPOF,single point of failure)
        - secondary 가 존재하긴 하지만, 동기화 시간차이가 존재하기 때문에 일부 데이터가 유실될 수 있는 문제가 있음
        - 세컨더리 네임노드가 즉각적인 대체 기능을 제공하지 않음 (고가용성을 제공하지는 않음)
        - 네임노드에 장애 발생시, 세컨더리 네임 노드를 복구하여 수동으로 네임노드를 새로 설정해야 함
    - Job Tracker 부하 발생
        - 클러스터 전체의 리소스 관리를 잡트래커가 하기 때문에, 부하가 많이 발생할 수 있다
    - Map Reduce 외에 다른 작업을 실행할 수 없는 문제

> 버전2
> 

<img src = "과제제출/김윤/asset/3.png" width="100%" height="100%">

- 개선점
    - 고가용성 지원 : namenode active-standby (이중화)
        - 두 네임노드가 메타데이터를 공유하여 실시간 대체 가능
        - 활성 네임노드가 editLog를  journalNode에 기록하고, 스탠바이 네임노드가 이를 읽어 실시간으로 동기화
    - 잡트래커, 태스크트래커 사라짐
    - 리소스매니저, 노드매니저 탄생
        - 리소스 매니저는 노드 매니저의 리소스를 파악하고, 최적의 작업을 실행하기 위한 데이터 노드 찾아준다
        - 노드 매니저의 컨테이너와 어플리케이션 마스터는 맵리듀스 뿐만 아니라 다양한 분산 어플리케이션을 실행할 수 있는 환경을 제공한다

> 버전 3
> 
- Erasure Coding
    - 기존 HDFS는 복제 개수가 3인 경우, 데이터를 저장할 때 데이터 저장 용량이 3배가 필요했다
    - 해리티블록? 을 사용해서 데이터 복사본의 사이즈를 줄여 저장소 효율을 높여준다
- YARN Timeline Service v2 도입
    - 타임라인 클라이언트에서 REST API를 통해 어플리케이션 마스터나 컨테이너에게 쿼리를 요청해서 어플리케이션 다양한 정보를 확인할 수 있는 기능
    - 확장성, 신뢰성 문제가 있었다
- Java 8
- name node 이중화 기능 강화
    - 버전 2에서는 두 개의 네임노드만 지원했다
    - 2개 이상 노드가 이중화를 지원하도록 기능 강화

# 2. why Hadoop?

<img src = "과제제출/김윤/asset/4.png" width="100%" height="100%">

- 확장성
- 비정형 데이터 처리도 잘한다
- 비용이 저렴
- hive, spark, hbase등과 호환성이 좋다

### why hadoop?

- hadoop 은 비정형 데이터를 포함한 빅데이터를 다루기 위한 가장 적절한 플랫폼
- hdfs로 데이터를 분산하여 안정적으로 저장할 수 있다
- 클라우드에서 hadoop 활용하면, 베어메탈 서버를 직접 관리할 필요가 없다

### Hadoop의 근원 : Google

<img src = "과제제출/김윤/asset/5.png" width="100%" height="100%">

- GFS 개념을 도입하여 개발한 분산 파일 시스템이 HDFS
- 구글이 데이터 작업을 병렬처리 하기 위해 개발한 프로그래밍 모델이 Map Reduce
    - Map 단계 + Reduce 단계를 병렬로 처리
    - Hadoop Mapreduce : 구글의 Map Reduce 동작을 그대로 Hadoop에 구현한 것
        - 대용량 데이터 분산 처리
- 구글이 개발한 데이터 저장 시스템 BigTable

### Hadoop의 역사

<img src = "과제제출/김윤/asset/6.png" width="100%" height="100%">
- 2002년 Apache Nutch : 웹사이트를 크롤링,색인화하는 검색엔진을 구축하는 프로젝트
    - 너무 비쌌다.
- 2003년 구글이 GFS 논문 발표
    - 대규모 분산 환경에서 대규모 데이터를 저장하는 환경 제안
    - Nutch 프로젝트에 GFS 개념 도입
- 2004년 구글 Map Reduce 논문 발표
    - 대규모 데이터를 처리하는 방안
    - Nutch 프로젝트에 Map Reduce 개념 도입
- 2006년 Nutch의 독립적인 서브 프로젝트로 Hadoop 시작
    - 처음엔 2-40개 노드의 클러스터를 가지고 시작됐다
    - Doug Cutting에 의해 1000개의 노드 클러스터에서 사용할 수 있게 됐다
- 2007년 야후에서 1000개의 노드 클러스터에서 Hadoop 사용 시작
- 2008년 Hadoop이 Apache top-level project가 됐다
    - 슈퍼컴퓨터 이김
- 2011년 Apache에서 Hadoop 1.0버전 release
- 2012년 Apache에서 Hadoop 2.0버전 release
- 2017년 Apache에서 Hadoop 3.0버전 release

### Hadoop이란

- 분산 환경에서 데이터를 저장하고 처리하는 오픈소스 소프트웨어
- Map Reduce 라는 데이터 처리 모델을 사용함으로써 여러 대의 컴퓨터를 통해 손쉽게 대규모 데이터 처리
- 분산 파일 시스템 HDFS에 데이터를 저장하여 처리
    - 여러 대의 서버에 데이터를 나눠서 저장한다
    - 이런 방식은 Disk I/O에 의해 성능 저하를 유발하기 때문에
    - 인메모리 기반 스파크가 등장해 보완한다

### Hadoop core

- 데이터 저장의 분산 파일 시스템 : HDFS
- 데이터 처리의 분산 데이터 처리 : Map Reduce
- 자원관리 : YARN

# 3. HDFS

### HDFS 모태 : GFS (google file system)

<img src = "과제제출/김윤/asset/7.png" width="100%" height="100%">

- 마스터 : GFS 마스터
- 워커 : chunk server
- HDFS 의 모태
    - 클라이언트는 마스터에게 파일 읽기나 쓰기를 요청하면
    - 마스터가 클라이언트와 가까운 chunk server의 정보를 클라이언트에게 전달하고
    - 클라이언트는 chunk server와 통신하여 I/O수행

### HDFS

- Hadoop Distributed File System
- Hadoop 생태계의 핵심 구성 요소 중 하나로, 데이터를 분산하여 저장하는 파일 시스템
- 마스터 슬레이브 구조로 하나의 네임노드와 여러 개의 데이터 노드로 구성
- 범용 하드웨어를 사용하여 분산 파일 시스템 구성
    - 범용 하드웨어로 구성된 클러스터에서 실행됨
    - 비싼 서버대신 범용 컴퓨터로도 구성 가능함
        - 고가 전문 서버 대신, 일반적인 사양으로도 클러스터를 구축할 수 있다
- 파일을 블록 단위로 저장
- 내고장성(fault-tolerance) 제공
    - 데이터를 복제하여 일부 장애가 발생하더라도 계속 서비스를 할 수 있는 내고장성 지원
- 확장성 제공

### HDFS 의 목표

<img src = "과제제출/김윤/asset/8.png" width="100%" height="100%">

- (1) 한 대의 고가의 서버보다 여러 대의 저가형 서버가 낫다
    - 확장성, 저렴, 분산처리(효율성)
- (2) 데이터는 분산저장한다
    - 하드웨어가 언제든지 죽을 수 있다
- (3) 시스템 확장이 쉬워야 한다

### 분산 파일 시스템

<img src = "과제제출/김윤/asset/9.png" width="100%" height="100%">

- 대용량 데이터를 한 번만 기록하고, 반복적으로 읽는 데이터 처리에 적합하다
- 블록은 독립적으로 존재하고, 여러 데이터노드에 복제된다.
    - 고정된 크기의 블록 사이즈로 저장된다
- 클라이언트는 네임스페이스를 통해 블록 위치를 찾을 수 있음
    - 클라이언트는 데이터노드에 직접 접근할 수 있다

### ⭐HDFS Architecture

<img src = "과제제출/김윤/asset/10.png" width="100%" height="100%">

- 마스터-슬레이브 구조로 하나의 네임노드와 여러 개의 데이터 노드로 구성
    - 장점 : 확장성이 좋다
        - 하나의 마스터 노드에 여러 슬레이브가 있기 때문에
        - 여러 대의 저가형 슬레이브 장비를 스케일 아웃하기 용이하다
- 마스터를 어떻게 관리할지가 핵심포인트 ⇒ 안정성 보장 필요
    - 마스터 노드에 장애가 발생한다면? ⇒ 클러스터 자체가 다운될 수도 있다
    - 마스터 노드는 메타데이터(파일 권한, 파일 위치 등)을 다 가지고 있다
    - 그래서 마스터 노드의 부하가 최소화 되도록 해주는 것이 중요하다
- 마스터 노드의 부하를 줄이는 방법 : 클라이언트를 통해 read,write작업이 진행된다
    - 마스터노드가 슬레이브 노드와 직접 데이터를 주고 받지 않고
    - 클라이언트가 슬레이브 노드와 데이터를 주고 받는다
    - 실제로 마스터노드는 메타데이터정보, 명령만 이루어진다

<img src = "과제제출/김윤/asset/11.png" width="100%" height="100%">

- NameNode : 마스터
- DataNode : 슬레이브
- 파일을 블록 단위로 분산하여 저장한다
    - 다른 데이터노드에 같은 블록이 복제되어 저장된다
- 데이터노드가 하는 일
    - 네임노드에게 하트비트 전송 (생존신고)
    - 네임노드에게 block report 전송 (내 노드에 정상 가동중인 블록 정보를 알려준다)
- 네임노드가 하는 일
    - 데이터노드에게 받은 정보를 기반으로 메타데이터 update
- 🚨흰색 block에 오류가 발생했다면 ? (자동)
    - 네임노드는 메타데이터를 통해 흰색 블록이 있는 다른 데이터노드를 찾는다
    - 다른 데이터노드에서 찾은 흰색 블록을 오류가 났다는 데이터노드에 복제하도록 한다
    - 복구 완료
- 😢 동일한 블록을 여러 개 저장하기 때문에 초반에 공격을 많이 받았다
    - 하둡은 여러 대의 저가형 장비를 사용하여 분산 저장/처리 하는 것을 목표로 한다
    - 비용적으로 계산해봤을 때, 하둡이 다른 상용 스토리지보다 훨씬 저렴하다
    - replication factor default 3
- 네임노드가 다운되면 클러스터가 다운되기 때문에, version2부터는 고가용성을 지원한다

### HDFS Block

<img src = "과제제출/김윤/asset/12.png" width="100%" height="100%">

- 하나의 파일을 여러 개의 블록으로 저장 ⇒ 블록 사이즈만큼 쪼개진다
- hadoop2부터 default 블록 사이즈가 128MB
- 파일 크기가 블록사이즈보다 작으면, 실제 크기만큼만 용량을 차지한다

> 블록 크기 분할의 이점
> 
- (1) 파일을 분할하여 저장하기 때문에, 디스크 크기보다 큰 파일을 저장할 수 있다
- (2) 파일 크기 제한을 두지 않아도 된다
- (3) 블록으로 나누지 않으면, 읽거나 저장하는데 오래 걸릴 수 있다.
- (4) 병렬 처리 지원 : 각 블록이 여러 노드에서 병렬로 처리
- (5) 고정된 메타정보 사이즈로 구현 용이
    - 블록 사이즈가 고정적이기 때문에, 메타정보 크기도 고정적이다
- (6) 내고장성(fault tolerance) 구현에 적합
    - 블록 단위로 데이터를 복제하여 저장한다
    - 하나의 노드가 고장나더라도 다른 노드에서 복제된 블록에서 데이터를 복구할 수 있다

> 왜 HDFS에서 블록 사이즈가 클까?
> 
- 일반적인 디스크 블록(4~64KB) 에 비해 크다
- (1) 대규모 파일 처리 최적화
    - HDFS는 대규모 데이터 저장하고 처리하기 위해 설계됐다
    - 블록 크기가 작으면 오히려 블록 관리 오버헤드 증가
- (2) 탐색 비용 최소화  && 메타데이터 관리 부담 완화
    - 블록이 커지면 블록의 시작점을 탐색하는 시간이 줄어들어, 데이터 전송에 더 시간을 쓸 수 있다
    - 블록마다 메타데이터가 있기 때문에, 블록이 크기가 크면 메타데이터 크기(총량) 감소한다
    - ⇒ 탐색 시간을 줄이면 더 많은 시간을 데이터 전송에 할애할 수 있다(네트워크 효율성 증가)

### NameNode 기능

<img src = "과제제출/김윤/asset/13.png" width="100%" height="100%">

- (1) 전체 HDFS에 대한 네임스페이스(메타데이터) 를 관리
    - 파일이 어느 블록에 저장되어 있는지, 어느 블록에 복제 되어 있는지 등 정보를 포함한다
- (2) 파일 시스템에 대한 Edit Log를 관리
    - 데이터 노드에서 발생한 데이터 변환 내역 (파일 생성, 삭제, 디렉토리 추가 등)
    - HDFS에서 파일 생성이나 삭제와 같은 트랜잭션 로그가 저장되어 있는 파일
- (3) 데이터노드로부터 fsimage를 관리
    - HDFS의 파일 시스템 상태를 포함한 스냅샷
    - 특정 시점에서 HDFS의 전체 디렉토리 구조와 파일 블록의 매핑 정보를 가지고 있다
    - HDFS를 재구동했을 때 혹은 장애가 발생했을 때 사용한다
- (4) 데이터 노드 관리
    - 데이터노드가 주기적으로 전달하는 하트비트를 통해 데이터노드의 동작 감지
    - block report 수신 (블록 상태, 위치 등)
- (5) 데이터 복제를 유지하기 위해 명령

### NameNode 메타데이터

- 메타데이터 관리
    - 파일 이름, 크기, 생성시간, 권한, 소유자 정보, 블록위치 등을 포함
    - 각 데이터노드에서 받은 메타데이터를 통합 관리
- 저장 위치
    - 사용자가 설정한 경로에 보관되며, 네임노드가 실행될 때 메모리에 로드
    - 수정사항은 메모리에 우선 적용되고, 주기적으로 Edits파일에 저장
- 메타데이터 파일 종류
    - fsimage :  네임스페이스와 블록 정보 저장
    - Edits 파일 : 파일 생성 및 삭제 트랜잭션 로그, 메모리에 저장 후 주기적으로 생성

### Secondary Namenode

- 네임노드의 standby 역할은 아니다
- 체크포인트 : 네임노드의 FsImage와 EditLog를 주기적으로 병합하는 업무
    - FsImage는 생성이 되면 변경되지 않는다
    - 그래서 HDFS가 동작하면서 변경되는 내용은 EditLog에 반영된다
    - EditLog는 크기제한이 없어서 계속 커질 수 있다 ⇒ 네임노드의 디스크 부족 문제 야기
    - 그래서 체크포인트를 통하여 최신 블록 상태를 파일로 생성하고, 디스크 부족 문제도 해결
- 주기적으로 네임노드의 FsImage를 백업
    - 네임노드에 문제가 생기면, 세컨더리 네임노드에 백업된 FsImage를 사용해서 복구를 할 수 있다
    - 백업 이후에 문제가 발생한다면? 데이터가 유실될 수 있음

> 체크포인트 과정
> 

<img src = "과제제출/김윤/asset/14.png" width="100%" height="100%">

체크포인트

1. HDFS가 시작되면 네임노드는 fsImage를 읽는다
2. 이후 HDFS에서 발생하는 모든 변경사항은 EditLog에 저장된다
3. 시간이 지날수록, EditLog가 커지면 네임노드의 메모리 및 처리속도에 영향을 미친다 (네임노드 트랜잭션이 빈번하면, 빠른 속도로 EditLog파일이 생성된다)

체크포인트

1. 세컨더리 네임노드가 네임노드에 EditLog에 Rolling 요청
    1. rolling은 현재 사용중인 EditLog파일을 닫고, 새로운 EditLog파일 생성하는 작업
    2. 기존 EditLog는 보존되며, 이후 변경사항은 새로운 EditLog에 저장된다
    3. 세컨더리 네임노드가 병합 작업을 수행하는 동안 editLog에 추가적인 변경사항이 기록되지 않게 하기 위해!
2. 세컨더리 네임노드에서 FsImage와 EditLog 복사
3. 세컨더리 네임노드는 fsImage와 EditLog를 병합하여 새로운 fsImage 생성
4. 완료된 fsImage를 네임노드로 업로드
5. 네임노드에서 fsImage 교체
    1. 이 과정에서 기존 editLog는 초기화

### DataNode

- HDFS데이터를 로컬 파일 시스템에 물리적으로 저장하는 역할
- HDFS의 구조나 동작에 대한 정보는 없음
- 일반적으로 JBOD 구성을 사용하며, RAID 구성을 하지 않음
    - Just a Branch Of Disks
    - 개별적인 디스크를 그대로 사용하는 것을 말한다
- 역할 : HDFS에서 파일을 블록 단위로 저장
- 하트비트 전송 : 현재 노드의 상태를 하트비트를 통해 네임노드에게 알려준다
    - 네임노드에서 하트비트를 못 받으면 데이터노드가 동작하지 않는다고 간주하여, 해당 데이터노드에 더 이상 데이터를 저장되지 않음
- 블록 리포트 전송 : 지정된 블록의 상태와 변경사항을 네임노드에 주기적으로 보고하여 메타데이터 갱신
    - 블록의 변경사항 체크후, 정상 블록 목록을 네임노드에게 전달
    - 네임노드는 블록 리포트를 통해 메타데이터 갱신

### HDFS 읽기 연산

<img src = "과제제출/김윤/asset/15.png" width="100%" height="100%">

<img src = "과제제출/김윤/asset/16.png" width="100%" height="100%">

- 클라이언트는 파일이 보관된 블록의 위치를 네임노드에게 요청
- 네임노드는 해당 블록의 모든 위치를 클라이언트에게 반환
- inputstream이 가장 가까운 데이터노드부터 반복적으로 read요청
- 다 읽었으면 close요청

### HDFS 쓰기 연산

<img src = "과제제출/김윤/asset/17.png" width="100%" height="100%">

<img src = "과제제출/김윤/asset/18.png" width="100%" height="100%">

- 네임노드에게 파일 정보를 전송하고 파일 블록을 써야 할 노드의 목록을 요청
- 네임노드에서 데이터를 쓸 데이터노드 리스트를 클라이언트에게 전달
- 클라이언트에서 데이터 노드에게 파일쓰기 요청
    - 설정된 replication factor를 기준으로 노드 간 복제가 진행됨
    - 복제가 완료되면 ack 정보를 받는다
    - 클라이언트에서 모든 데이터 노드가 복제 되었다는 ack를 받으면, 네임노드에게 완료됐다고 알려준다

### HDFS 추가 특징

- 블록 캐싱 기능 제공
    - 데이터 노드에 저장된 데이터 중에서 자주 읽는 블록을
    - 블록 캐시로 데이터 노드의 메모리에 명시적으로 캐싱할 수 있다
    - 파일 단위로도 캐싱 가능
- HDFS Federation 지원
    - 네임노드는 파일정보를 메타데이터를 통해 관리하고
    - 메타데이터는 메모리를 통해 관리된다
    - 파일이 많아지게되면 메타데이터를 위한 메모리 사용량 증가
    - 버전2부터 Federation 지원
    - 네임스페이스 단위로 네임노드를 등록하여 사용한다
    - 각 디렉토리마다 네임노드를 실행해서 관리
        - 기존 네임노드가 가지는 메모리 문제 해결
- 고가용성 지원
    - 액티브-스탠바이 노드
    - 스탠바이는 액티브와 동일한 메타데이터 유지
    - zookeeper를 통해 HA지원

### HA아키텍쳐에서는 더 이상 세컨더리 네임노드가 필요하지 않다

- Active Namenode, Standby Namenode, JournalNode
- EditLog크기 문제를 해결하기 위해 JournalNode 도입
1. EditLog 기록 및 JournalNode 저장:
    - 활성 네임노드는 메타데이터 변경 사항(파일 생성, 삭제 등)을 **EditLog에 기록**하고, 이를 **JournalNode 클러스터**에 전송합니다.
    - **활성 네임노드 자체에는 EditLog가 저장되지 않으며**, EditLog는 JournalNode에서 관리됩니다.
2. **EditLog 실시간 반영**:
    1. 스탠바이 노드는 Journal Node에서 실시간으로 EditLog를 읽어와 , 자신의 메모리와 fsImage에 반영
        1. 병합이 완료되면 스탠바이 네임노드는 JournalNode에 병합된 EditLog를 삭제하도록 요청합니다.
        2. 이 과정에서 JournalNode는 병합된 EditLog 항목만 삭제하고, 새로운 EditLog는 그대로 유지합니다.
        3. 활성 네임노드는 이후의 변경 사항을 새로운 EditLog 파일에 기록하기 시작합니다.
    2. 이를 통해 스탠바이 노드는 활성노드와 동일한 최신 메타데이터 유지
3. 체크포인트 수행
    1. 스탠바이 노드는 주기적으로 EditLog와 fsImage를 병합하여 새로운 fsImage생성
    2. 병합된 fsImage는 스탠바이 노드에만 저장된다

> 장애발생시
> 
1. zookeeper가 활성 노드 상태를 지속적으로 감시하여, 장애 발생하면 스탠바이 노드를 활성 노드로 승격
2. 스탠바이 네임노드 활성화
    - 스탠바이 네임노드는 이미 JournalNode에서 최신 EditLog를 읽어와 메모리와 fsimage에 반영했기 때문에, 활성 네임노드와 동일한 최신 상태를 유지합니다.
